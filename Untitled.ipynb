{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f9f1505-4be3-4d81-8dd7-8d8d22d0650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict, deque\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from Rubiks.state import State\n",
    "from Rubiks.utils import move, num_pieces_correct_side, num_solved_sides, shuffle\n",
    "from Rubiks.agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "970ada46-8d49-48d7-985f-283ef18539c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pattern database\n",
    "pattern_database = defaultdict(lambda: float('inf'))\n",
    "\n",
    "# Function to populate the pattern database with heuristic values, with a depth limit\n",
    "def populate_pattern_database(depth_limit=7):\n",
    "    initial_state = State()\n",
    "    queue = deque([(initial_state, 0)])\n",
    "    visited = set()\n",
    "    \n",
    "    while queue:\n",
    "        current_state, depth = queue.popleft()\n",
    "        state_hash = hash(current_state)\n",
    "        \n",
    "        if state_hash in visited or depth > depth_limit:\n",
    "            continue\n",
    "        \n",
    "        visited.add(state_hash)\n",
    "        pattern_database[state_hash] = depth\n",
    "        \n",
    "        for action in current_state.actions:\n",
    "            next_state = move(current_state, action)\n",
    "            next_state_hash = hash(next_state)\n",
    "            if next_state_hash not in visited:\n",
    "                queue.append((next_state, depth + 1))\n",
    "\n",
    "# Populate the pattern database (this may take some time)\n",
    "populate_pattern_database(depth_limit=7)\n",
    "\n",
    "# Convert defaultdict to dict before saving\n",
    "pattern_database = dict(pattern_database)\n",
    "\n",
    "# Save the pattern database to a file\n",
    "with open('pattern_database.pkl', 'wb') as f:\n",
    "    pickle.dump(pattern_database, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2681ae37-be03-4154-9835-a32b05c16e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pattern database from a file\n",
    "with open('pattern_database.pkl', 'rb') as f:\n",
    "    pattern_database = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "836c79d8-3a33-4379-9a76-411a588fd52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-values\n",
    "Q = defaultdict(lambda: 0.0)\n",
    "\n",
    "# Parameters\n",
    "alpha = 0.1  # You can try values like 0.01, 0.05, 0.1, 0.2\n",
    "gamma = 0.9  # You can try values like 0.8, 0.9, 0.95, 0.99\n",
    "initial_epsilon = 1.0\n",
    "epsilon_decay = 0.995  # You can try values like 0.99, 0.995, 0.999\n",
    "min_epsilon = 0.01  # Keep this to ensure some level of exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dea8390-3bf2-4ac4-884a-ad0a2bd8d703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(state):\n",
    "    if state.isGoalState():\n",
    "        return 100\n",
    "    solved_sides = num_solved_sides(state)\n",
    "    correct_pieces = num_pieces_correct_side(state)\n",
    "    reward = -1 + solved_sides * 10 + correct_pieces * 0.1  # Adjust these weights as needed\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "422258ea-a293-4098-9e8a-d7006365770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_update(state, action, next_state, reward):\n",
    "    state_action = (hash(state), action)\n",
    "    best_next_action = max(Q[(hash(next_state), a)] for a in next_state.actions)\n",
    "    Q[state_action] = Q[state_action] + alpha * (reward + gamma * best_next_action - Q[state_action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3243762-6eaa-4098-ba1a-91a44638a855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, epsilon):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(state.actions)  # Explore: choose a random action\n",
    "    else:\n",
    "        state_actions = [(Q[(hash(state), action)], action) for action in state.actions]\n",
    "        return max(state_actions, key=lambda x: x[0])[1]  # Exploit: choose the best action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3d2aa7d-f5ee-46cb-9c06-32636226b069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: solved in 1000 steps, total reward: 1689.999999999999, epsilon: 0.995, scramble_moves: 4\n",
      "Episode 100: solved in 1000 steps, total reward: 1711.3999999999985, epsilon: 0.6027415843082742, scramble_moves: 4\n",
      "Episode 200: solved in 1000 steps, total reward: 1662.3999999999996, epsilon: 0.36512303261753626, scramble_moves: 5\n",
      "Episode 300: solved in 1000 steps, total reward: 1589.9999999999943, epsilon: 0.2211807388415433, scramble_moves: 5\n",
      "Episode 400: solved in 1000 steps, total reward: 1375.0000000000036, epsilon: 0.13398475271138335, scramble_moves: 6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Train the agent with improved parameters and strategy\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_scramble_moves\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_scramble_moves\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscramble_increase_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 22\u001b[0m, in \u001b[0;36mtrain_agent\u001b[0;34m(episodes, initial_scramble_moves, max_scramble_moves, scramble_increase_interval)\u001b[0m\n\u001b[1;32m     19\u001b[0m solved \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state\u001b[38;5;241m.\u001b[39misGoalState() \u001b[38;5;129;01mand\u001b[39;00m steps \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1000\u001b[39m:  \u001b[38;5;66;03m# Limit steps to prevent infinite loop\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m move(state, action)\n\u001b[1;32m     24\u001b[0m     reward \u001b[38;5;241m=\u001b[39m reward_function(next_state)\n",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m, in \u001b[0;36mchoose_action\u001b[0;34m(state, epsilon)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m random\u001b[38;5;241m.\u001b[39mchoice(state\u001b[38;5;241m.\u001b[39mactions)  \u001b[38;5;66;03m# Explore: choose a random action\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m     state_actions \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(state_actions, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m])[\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m random\u001b[38;5;241m.\u001b[39mchoice(state\u001b[38;5;241m.\u001b[39mactions)  \u001b[38;5;66;03m# Explore: choose a random action\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m     state_actions \u001b[38;5;241m=\u001b[39m [(Q[(\u001b[38;5;28mhash\u001b[39m(state), action)], action) \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m state\u001b[38;5;241m.\u001b[39mactions]\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(state_actions, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m])[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/RL_Project/Rubiks/state.py:78\u001b[0m, in \u001b[0;36mState.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mhash\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__str__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/RL_Project/Rubiks/state.py:74\u001b[0m, in \u001b[0;36mState.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFRONT\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__front__) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBACK\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__back__) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLEFT\u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__left__) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRIGHT\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__right__) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTOP\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__top__) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBOTTOM\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__bottom__)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_agent(episodes=1500, initial_scramble_moves=3, max_scramble_moves=10, scramble_increase_interval=200):\n",
    "    initial_state = State()\n",
    "    epsilon = initial_epsilon\n",
    "    scramble_moves = initial_scramble_moves\n",
    "    \n",
    "    # Metrics tracking\n",
    "    rewards = []\n",
    "    steps_list = []\n",
    "    solved_episodes = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        # Gradually increase scramble complexity\n",
    "        if episode % scramble_increase_interval == 0 and scramble_moves < max_scramble_moves:\n",
    "            scramble_moves += 1\n",
    "        \n",
    "        state = shuffle(initial_state, n=scramble_moves)  # Start with a scrambled state\n",
    "        steps = 0\n",
    "        total_reward = 0\n",
    "        solved = False\n",
    "        \n",
    "        while not state.isGoalState() and steps < 1000:  # Limit steps to prevent infinite loop\n",
    "            action = choose_action(state, epsilon)\n",
    "            next_state = move(state, action)\n",
    "            reward = reward_function(next_state)\n",
    "            q_learning_update(state, action, next_state, reward)\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "            total_reward += reward\n",
    "        \n",
    "        if state.isGoalState():\n",
    "            solved = True\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "        steps_list.append(steps)\n",
    "        solved_episodes.append(solved)\n",
    "        \n",
    "        # Decay epsilon\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            print(f'Episode {episode}: solved in {steps} steps, total reward: {total_reward}, epsilon: {epsilon}, scramble_moves: {scramble_moves}')\n",
    "    \n",
    "    # Plot average rewards\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(np.convolve(rewards, np.ones(100)/100, mode='valid'))\n",
    "    plt.title('Average Reward Over Time')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot success rate\n",
    "    success_rate = np.convolve(solved_episodes, np.ones(100)/100, mode='valid')\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(success_rate)\n",
    "    plt.title('Success Rate Over Time')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Success Rate')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot steps to solve\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(steps_list)\n",
    "    plt.title('Steps to Solve Over Time')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Steps')\n",
    "    plt.show()\n",
    "\n",
    "# Train the agent with improved parameters and strategy\n",
    "train_agent(episodes=1500, initial_scramble_moves=3, max_scramble_moves=10, scramble_increase_interval=200)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
