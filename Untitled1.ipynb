{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2479d0d0-25d2-4b4c-96fb-b5eb8b7d42fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict, deque\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from Rubiks.state import State\n",
    "from Rubiks.utils import move, shuffle\n",
    "from Rubiks.agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2fbccf0-f75d-459d-91c6-b761effe113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pattern database\n",
    "pattern_database = defaultdict(lambda: float('inf'))\n",
    "\n",
    "# Function to populate the pattern database with heuristic values, with a depth limit\n",
    "def populate_pattern_database(depth_limit=7):\n",
    "    initial_state = State()\n",
    "    queue = deque([(initial_state, 0)])\n",
    "    visited = set()\n",
    "    \n",
    "    while queue:\n",
    "        current_state, depth = queue.popleft()\n",
    "        state_hash = hash(current_state)\n",
    "        \n",
    "        if state_hash in visited or depth > depth_limit:\n",
    "            continue\n",
    "        \n",
    "        visited.add(state_hash)\n",
    "        pattern_database[state_hash] = depth\n",
    "        \n",
    "        for action in current_state.actions:\n",
    "            next_state = move(current_state, action)\n",
    "            next_state_hash = hash(next_state)\n",
    "            if next_state_hash not in visited:\n",
    "                queue.append((next_state, depth + 1))\n",
    "\n",
    "# Populate the pattern database (this may take some time)\n",
    "populate_pattern_database(depth_limit=7)\n",
    "\n",
    "# Convert defaultdict to dict before saving\n",
    "pattern_database = dict(pattern_database)\n",
    "\n",
    "# Save the pattern database to a file\n",
    "with open('pattern_database.pkl', 'wb') as f:\n",
    "    pickle.dump(pattern_database, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd7879f0-7436-4542-839b-bab7795e915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pattern database from a file\n",
    "with open('pattern_database.pkl', 'rb') as f:\n",
    "    pattern_database = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b71f5a57-2637-465e-9fe6-35df0b865a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-values\n",
    "Q = defaultdict(lambda: 0.0)\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.95\n",
    "initial_epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "min_epsilon = 0.01\n",
    "replay_buffer = deque(maxlen=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8ab624c-656e-4f99-885d-3a33b6060e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(state):\n",
    "    if state.isGoalState():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27caf7b0-b3e8-426d-919e-68ba8219d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_update(state, action, next_state, reward):\n",
    "    state_action = (hash(state), action)\n",
    "    best_next_action = max(Q[(hash(next_state), a)] for a in next_state.actions)\n",
    "    Q[state_action] = Q[state_action] + alpha * (reward + gamma * best_next_action - Q[state_action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c67ba521-b49e-4260-8605-5089f589463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, epsilon):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(state.actions)  # Explore: choose a random action\n",
    "    else:\n",
    "        state_actions = [(Q[(hash(state), action)], action) for action in state.actions]\n",
    "        return max(state_actions, key=lambda x: x[0])[1]  # Exploit: choose the best action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116a3fd4-2f64-4094-b08d-7a134bd8ed7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: solved in 1000 steps, total reward: 0, epsilon: 0.995, scramble_moves: 4\n",
      "Episode 100: solved in 1000 steps, total reward: 0, epsilon: 0.6027415843082742, scramble_moves: 4\n",
      "Episode 200: solved in 1000 steps, total reward: 0, epsilon: 0.36512303261753626, scramble_moves: 5\n"
     ]
    }
   ],
   "source": [
    "def train_agent(episodes=1500, initial_scramble_moves=3, max_scramble_moves=10, scramble_increase_interval=200):\n",
    "    initial_state = State()\n",
    "    epsilon = initial_epsilon\n",
    "    scramble_moves = initial_scramble_moves\n",
    "    \n",
    "    # Metrics tracking\n",
    "    rewards = []\n",
    "    steps_list = []\n",
    "    solved_episodes = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        # Gradually increase scramble complexity\n",
    "        if episode % scramble_increase_interval == 0 and scramble_moves < max_scramble_moves:\n",
    "            scramble_moves += 1\n",
    "        \n",
    "        state = shuffle(initial_state, n=scramble_moves)  # Start with a scrambled state\n",
    "        steps = 0\n",
    "        total_reward = 0\n",
    "        solved = False\n",
    "        \n",
    "        while not state.isGoalState() and steps < 1000:  # Limit steps to prevent infinite loop\n",
    "            action = choose_action(state, epsilon)\n",
    "            next_state = move(state, action)\n",
    "            reward = reward_function(next_state)\n",
    "            q_learning_update(state, action, next_state, reward)\n",
    "            \n",
    "            # Store experience in replay buffer\n",
    "            replay_buffer.append((state, action, reward, next_state))\n",
    "            \n",
    "            state = next_state\n",
    "            steps += 1\n",
    "            total_reward += reward\n",
    "        \n",
    "        if state.isGoalState():\n",
    "            solved = True\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "        steps_list.append(steps)\n",
    "        solved_episodes.append(solved)\n",
    "        \n",
    "        # Decay epsilon\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            print(f'Episode {episode}: solved in {steps} steps, total reward: {total_reward}, epsilon: {epsilon}, scramble_moves: {scramble_moves}')\n",
    "    \n",
    "    # Plot average rewards\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(np.convolve(rewards, np.ones(100)/100, mode='valid'))\n",
    "    plt.title('Average Reward Over Time')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot success rate\n",
    "    success_rate = np.convolve(solved_episodes, np.ones(100)/100, mode='valid')\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(success_rate)\n",
    "    plt.title('Success Rate Over Time')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Success Rate')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot steps to solve\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(steps_list)\n",
    "    plt.title('Steps to Solve Over Time')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Steps')\n",
    "    plt.show()\n",
    "\n",
    "# Initialize Q-values\n",
    "Q = defaultdict(lambda: 0.0)\n",
    "\n",
    "# Train the agent with improved parameters and strategy\n",
    "train_agent(episodes=1500, initial_scramble_moves=3, max_scramble_moves=10, scramble_increase_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023df3d2-44bf-455c-8754-09c968948efa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
